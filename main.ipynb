{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------#\n",
    "#          Universidade Federal de Santa Catarina - UFSC/Florianópolis\n",
    "#             Programa de Pós Graduação em Engenharia Elétrica\n",
    "#                           Engenharia Biomédica\n",
    "#                    Código da Dissertação de Mestrado\n",
    "#                     Discente: Vinícius Rodrigues Zanon\n",
    "#                       Orientador: Cesar Ramos Rodrigues\n",
    "#\n",
    "#    Título: Estudo da aplicação das redes neurais pulsadas em sistemas personalizados de\n",
    "#            detecção de crises epilépticas com base em sinais cardíacos. \n",
    "#\n",
    "#\n",
    "#  - Todos os direitos de código são reservados ao autor discente deste trabalho, sendo\n",
    "#    passível de processos extrajudicias se não for devidamente referenciado.\n",
    "#\n",
    "#   Citação:\n",
    "#\n",
    "#   @monography{ZANON2024,\n",
    "#       author       = {Vinicius R. {Zanon}}, \n",
    "#       title        = {Estudo da aplicação das redes neurais pulsadas em sistemas personalizados\n",
    "#                       de detecção de crises epilépticas com base em sinais cardíacos},\n",
    "#       school       = {Universidade Federal de Santa Catarina --- UFSC},\n",
    "#       year         = {2024},\n",
    "#       address      = {Florianópolis},\n",
    "#       pages        = {55},\n",
    "#       type         = {Mestrado},\n",
    "#       note         = {Programa de Pós-graduação em Engenharia Elétrica - PPGEEL}\n",
    "#   }\n",
    "#\n",
    "#--------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------#\n",
    "#                   Importação de Bibliotecas\n",
    "#--------------------------------------------------------------#\n",
    "import collections\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install imblear\n",
    "!{sys.executable} -m pip install tensorflow_addons\n",
    "\n",
    "import seaborn as sns\n",
    "import imblearn\n",
    "import tensorflow_addons as tfa\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import tensorflow_addons as tfa\n",
    "#--------------------------------------------------------------#\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import keras_spiking\n",
    "import nengo_loihi\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "tf.get_logger().addFilter(lambda rec: \"Tracing is expensive\" not in rec.msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------#\n",
    "#                Declaração de Funções/Métodos\n",
    "#--------------------------------------------------------------#\n",
    "\n",
    "# Função para criar o modelo, requerido pelo KerasClassifier\n",
    "def create_model(init_mode='glorot_uniform', activation = 'softmax',\n",
    "                 loss = ['categorical_crossentropy'], dropout_rate=0.0,\n",
    "                 weight_constraint=1                 \n",
    "                 ):  \n",
    "  \n",
    "  global layer1, layer2, layer3, layer4, layerX\n",
    "  \n",
    "  layer1 = tf.keras.layers.Dense(128, input_dim = 3, activation = 'relu', kernel_constraint=maxnorm(weight_constraint), name=\"layer1\")\n",
    "  layer2 = tf.keras.layers.Dropout(dropout_rate, name=\"layer2\")\n",
    "  layer3 = tf.keras.layers.Dense(64, activation = 'relu', kernel_constraint=maxnorm(weight_constraint), name=\"layer3\")\n",
    "  layer4 = tf.keras.layers.Dense(3, activation = activation, name=\"layer4\")\n",
    "  layerX = tf.keras.layers.BatchNormalization(momentum=0.99)\n",
    "  \n",
    "  model = Sequential([layer1, layer2, layer3, layer4, layerX])\n",
    "\n",
    "  # Antes de treinarmos o modelo, é necessário compilarmos com o otimizador e a função de perda\n",
    "  model.compile(optimizer = 'adam', loss = loss, metrics = ['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARAÇÃO DA BASE DE DADOS EM PACIENTES ESPECÍFICOS (EXECUTAR APENAS UMA VEZ)\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Leitura do arquivo CSV\n",
    "nome_do_arquivo = 'C:/Users/zanon/Desktop/CODIGO/Codigos-Vini-Modificado/Sinais/teste/seizures_siena_post_norm_treino_completo.csv'\n",
    "dados = pd.read_csv(nome_do_arquivo, sep=';')\n",
    "\n",
    "# Diretório de destino\n",
    "diretorio_destino = 'C:\\\\Users\\\\zanon\\\\Desktop\\\\CODIGO\\\\Codigos-Vini-Modificado\\\\Sinais\\\\teste\\\\pacientes'\n",
    "\n",
    "# Inicialização de variáveis\n",
    "paciente_atual = -1\n",
    "dados_paciente = []\n",
    "\n",
    "# Iteração sobre as linhas do DataFrame\n",
    "for index, row in dados.iterrows():\n",
    "    # Verifica se a coluna 'Intervalo observado [min]' é igual a 1 para indicar um novo paciente\n",
    "    if row['Intervalo observado [min]'] == 1:\n",
    "        # Incrementa o número do paciente antes de salvar os dados do paciente anterior\n",
    "        paciente_atual += 1\n",
    "\n",
    "        # Salva os dados do paciente anterior, se existirem\n",
    "        if dados_paciente:\n",
    "            df_paciente = pd.DataFrame(dados_paciente)\n",
    "            caminho_arquivo = os.path.join(diretorio_destino, f'paciente_{paciente_atual}.csv')\n",
    "            df_paciente.to_csv(caminho_arquivo, index=False, sep=';')\n",
    "        \n",
    "        # Reinicia a lista de dados do paciente\n",
    "        dados_paciente = []\n",
    "\n",
    "    # Adiciona os dados à lista do paciente atual\n",
    "    dados_paciente.append(row)\n",
    "\n",
    "# Salva os dados do último paciente\n",
    "if dados_paciente:\n",
    "    df_paciente = pd.DataFrame(dados_paciente)\n",
    "    caminho_arquivo = os.path.join(diretorio_destino, f'paciente_{paciente_atual}.csv')\n",
    "    df_paciente.to_csv(caminho_arquivo, index=False, sep=';')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------#\n",
    "#                      Programa Principal\n",
    "#--------------------------------------------------------------#\n",
    "\n",
    "# Importação dos dados com url\n",
    "\n",
    "uri = \"C:/Users/zanon/Desktop/CODIGO/Codigos-Vini-Modificado/Sinais/teste/pacientes/paciente_1.csv\"    #dataset específico paciente X\n",
    "nome_pasta1 = 'logs_with_cross_validation_P1/data-'\n",
    "nome_pasta2 = 'logs_with_cross_validation_P1/hparam_tuning/'\n",
    "type_search_flag = 1  \n",
    "\n",
    "dados = pd.read_csv(uri, sep=';', low_memory=False)\n",
    "\n",
    "print('\\nIntervalos observados por classe')\n",
    "print(dados[\"Crise\"].value_counts())\n",
    "\n",
    "SEED = 5\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "troca = {'interictal' : 0,\n",
    "         'preictal'   : 1,\n",
    "         'ictal'      : 2,\n",
    "         'postictal'  : 0}\n",
    "\n",
    "dados['Crise_idx'] = dados.Crise.map(troca)\n",
    "\n",
    "# separando os dados para features e para classe\n",
    "x_dados = dados[ [\"MediaRR [ms]\",  # Average RR-intervals\n",
    "                  \"CSI_mod [a.u]\", # Cardiac Vagal Index\n",
    "                  \"AT [a.u]\"       # Hjorth Activity\n",
    "                  ] ]\n",
    "\n",
    "y_dados = dados[\"Crise\"]\n",
    "print('\\nCabeçalho do x_dados')\n",
    "print(x_dados.head())\n",
    "\n",
    "print('\\nCabeçalho do y_dados')\n",
    "print(y_dados.head())\n",
    "\n",
    "print('\\nForma dos Dados')\n",
    "print('x_dados: ', x_dados.shape)\n",
    "print('y_dados: ', y_dados.shape)\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x_dados, y_dados, test_size=0.2, random_state=42) # 80 - train : 20 - test   \n",
    "\n",
    "print('x_train_val | y_train_val',x_train_val.shape, y_train_val.shape)\n",
    "print('x_test | y_test', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.subplot(4,1,1) \n",
    "plt.plot(x_dados['MediaRR [ms]'], color='DeepPink')\n",
    "plt.ylabel('MediaRR [ms]')\n",
    "plt.grid()\n",
    "plt.subplot(4,1,2) \n",
    "plt.plot(x_dados['CSI_mod [a.u]'], color='DarkSlateGray')\n",
    "plt.ylabel('CSI_mod [a.u]')\n",
    "plt.grid()\n",
    "plt.subplot(4,1,3) \n",
    "plt.plot(x_dados['AT [a.u]'], color='Green')\n",
    "plt.ylabel('AT [a.u]')\n",
    "plt.grid()\n",
    "plt.subplot(4,1,4) \n",
    "plt.plot(y_dados, label='Output', color='cornflowerblue')\n",
    "plt.ylabel('Períodos')\n",
    "plt.xlabel('Tempo [s]')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,20)) \n",
    "plt.subplot(5,2,1) \n",
    "sns.distplot(dados['MediaRR [ms]'],color='DeepPink') \n",
    "plt.subplot(5,2,2) \n",
    "sns.boxplot(dados['MediaRR [ms]'],color='DeepPink') \n",
    "plt.subplot(5,2,3) \n",
    "sns.distplot(dados['CSI_mod [a.u]'],color='DarkSlateGray') \n",
    "plt.subplot(5,2,4) \n",
    "sns.boxplot(dados['CSI_mod [a.u]'],color='DarkSlateGray') \n",
    "plt.subplot(5,2,5) \n",
    "sns.distplot(dados['AT [a.u]'],color='Green') \n",
    "plt.subplot(5,2,6) \n",
    "sns.boxplot(dados['AT [a.u]'],color='Green') \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando a correlaçao entre as variaveis via método de pearson\n",
    "\n",
    "corr = x_dados.corr(method='pearson')\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(corr, vmin = -1, vmax=1, linewidth=0.001, square=True, annot=True, cmap='YlGnBu', linecolor='white')\n",
    "plt.title('Correlação entre as métricas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar a assimetria (se tiver: excluir variáveis muito assimétricas - valores muito altos ou muito baixos)\n",
    "from scipy.stats import skew\n",
    "skewed_feats = x_dados.apply(lambda x: skew(x.dropna()))\n",
    "print(skewed_feats.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar features constantes (com variância zero)\n",
    "constant_features = [feat for feat in x_dados.columns if x_dados[feat].std() == 0]\n",
    "print(constant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronização / Regularização dos Dados de Treinamento\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_dados)\n",
    "x_dados = pd.DataFrame(sc.transform(x_dados), columns=x_dados.columns)\n",
    "print('Média x_dados: \\n', x_dados.mean()) # media deve ser 0 \n",
    "print('\\nDesvio Padrão x_dados: \\n', x_dados.var()) # desvio padrão deve ser 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dados_count = dados['Intervalo observado [min]']\n",
    "print(type(x_dados_count))\n",
    "\n",
    "contagem_pacientes = x_dados_count.value_counts()\n",
    "print(\"Intervalo analisado [min]    [count]:\", contagem_pacientes.head(1))\n",
    "print(\"\\nNumero de pacientes: \", contagem_pacientes.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Modelo Clássico de Rede\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Avalia consumo de energia do modelo atual.\n",
    "energy = keras_spiking.ModelEnergy(model)\n",
    "energy.summary(print_warnings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def run_train(run_dir, hparams):\n",
    "    \n",
    "    #Adjust data train\n",
    "    x_dados_ = x_train_val\n",
    "    y_dados_ = y_train_val\n",
    "\n",
    "    #'Ictal', 'Interictal', 'Pre-Ictal' -> 0, 1 , 2\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(y_dados_)\n",
    "    y_dados_ = label_encoder.transform(y_dados_)\n",
    "\n",
    "    label_encoder_test = preprocessing.LabelEncoder()\n",
    "    label_encoder_test.fit(y_test)\n",
    "    y_test_ = label_encoder_test.transform(y_test)\n",
    "\n",
    "    x_dados_ = x_dados_.to_numpy()\n",
    "    x_test_ = x_test.to_numpy()\n",
    "\n",
    "    x_test_ = x_test_.reshape((x_test_.shape[0], 1, -1))\n",
    "    y_test_ = y_test_.reshape((y_test_.shape[0], 1, -1))\n",
    "\n",
    "    x_dados_ = x_dados_.reshape((x_dados_.shape[0], 1, -1))\n",
    "    y_dados_ = y_dados_.reshape((y_dados_.shape[0], 1, -1))\n",
    "\n",
    "    x_dados_ = np.asarray(x_dados_).astype(np.float32)\n",
    "    y_dados_ = np.asarray(y_dados_).astype(np.int64)\n",
    "\n",
    "    x_test_ = np.asarray(x_test_).astype(np.float32)\n",
    "    y_test_ = np.asarray(y_test_).astype(np.int64)\n",
    "\n",
    "    print('RUN TRAIN:')\n",
    "    print('Shape x_dados_train: ' , x_dados_.shape)\n",
    "    print('Shape y_dados_train: ', y_dados_.shape)\n",
    " \n",
    "    model = create_model(init_mode='glorot_uniform', activation = 'softmax', dropout_rate=hparams[HP_DROPOUT_RATE],\n",
    "                 weight_constraint=hparams[HP_W_CONSTRAINT])\n",
    "    \n",
    "    # Convert model simple way\n",
    "    aux_act = hparams[HP_ACTIVATION]\n",
    "    if aux_act == 'LIF':\n",
    "        act = nengo.LIF()\n",
    "    elif aux_act == 'Izhikevich':\n",
    "        act = nengo.Izhikevich()\n",
    "    elif aux_act == 'SpikingRectifiedLinear':\n",
    "        act = nengo.SpikingRectifiedLinear()\n",
    "    elif aux_act == 'RectifiedLinear':\n",
    "        act = nengo.RectifiedLinear()\n",
    "    else :\n",
    "        act = nengo.RectifiedLinear()\n",
    "\n",
    "    \n",
    "    converter = nengo_dl.Converter(model, swap_activations={tf.keras.activations.relu: act},\n",
    "                                   scale_firing_rates=hparams[HP_SCALE_FIRING_RATES],\n",
    "                                   synapse=hparams[HP_SYNAPSE])\n",
    "    \n",
    "    \n",
    "    #print(converter.verify()) # if return equals 'true' it means it's the correct conversion\n",
    "\n",
    "    # Create data dictionaries to input data (train, validation and test)\n",
    "    # OBS: converter.model\n",
    "    train_inputs = {converter.inputs[converter.model.layers[0].get_output_at(-1)]: x_dados_[train]}\n",
    "    train_targets = {converter.outputs[converter.model.output]: y_dados_[train]}\n",
    "\n",
    "    val_inputs = {converter.inputs[converter.model.layers[0].get_output_at(-1)]: x_dados_[validation]}\n",
    "    val_targets = {converter.outputs[converter.model.output]: y_dados_[validation]}\n",
    "\n",
    "    test_inputs = {converter.inputs[converter.model.layers[0].get_output_at(-1)]: x_test_}\n",
    "    test_targets = {converter.outputs[converter.model.output]: y_test_}\n",
    "\n",
    "    # get input/output objects\n",
    "    nengo_input = converter.inputs[converter.model.layers[0].get_output_at(-1)]\n",
    "    nengo_output = converter.outputs[converter.model.output]\n",
    " \n",
    "    # Chose the optimizer and learning rate\n",
    "    learn_rate = hparams[HP_LR]\n",
    "    ep = hparams[HP_EPOCHS]\n",
    "    \n",
    "    def get_callbacks(log_dir):\n",
    "            return [\n",
    "                #tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "                hp.KerasCallback(log_dir, hparams),\n",
    "              ]\n",
    "    \n",
    "    # Creates names to save the parameters of trainig and logs to tensorboard\n",
    "    log_dir = nome_pasta1 + run_dir\n",
    "    \n",
    "    # Training the model\n",
    "    do_training = True\n",
    "    if do_training:\n",
    "        print('#--------Training---------#')\n",
    "        mini_batch = hparams[HP_BATCH_SIZE]\n",
    "        # set some options to speed up simulation\n",
    "        with converter.net:\n",
    "            nengo_dl.configure_settings(stateful=False)\n",
    "       \n",
    "        ## OBS:Use converter.net\n",
    "        # run training nengo converter network\n",
    "        with nengo_dl.Simulator(converter.net, minibatch_size=mini_batch) as sim:\n",
    "            # Evaluate before training (need compile before)\n",
    "            # Evaluate: Compute the loss and metric values for the network.\n",
    "            sim.compile(\n",
    "                loss={nengo_output: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "                metrics={nengo_output: [f1_metric]})\n",
    "                #metrics={nengo_output: [tf.metrics.sparse_categorical_accuracy, f1_metric]})\n",
    "            \n",
    "            # Evaluate before training\n",
    "            # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "            evaluate_b = sim.evaluate(test_inputs, test_targets, verbose=1)\n",
    "\n",
    "            # Get start time training\n",
    "            start_train_time = time.time()\n",
    "            \n",
    "            # Run training\n",
    "            sim.compile(\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
    "                loss={nengo_output: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "                metrics={nengo_output: [f1_metric]})\n",
    "            \n",
    "            history = sim.fit(train_inputs, train_targets, \n",
    "                validation_data=(val_inputs, val_targets),\n",
    "                epochs=ep,\n",
    "                callbacks = get_callbacks(log_dir)\n",
    "            )\n",
    "            \n",
    "            # Get end time training\n",
    "            elapsed_train_time = time.time() - start_train_time\n",
    "            elapsed_train_time_m = elapsed_train_time / 60\n",
    "\n",
    "            # Save the parameters to file\n",
    "            params_file = sim.save_params(log_dir)\n",
    "\n",
    "            # Evaluate after training\n",
    "            sim.compile(\n",
    "                loss={nengo_output: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "                metrics={nengo_output: [f1_metric]})\n",
    "            \n",
    "            # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "            evaluate_a = sim.evaluate(test_inputs, test_targets, verbose = 1)\n",
    "\n",
    "    else:\n",
    "        # download pretrained weights\n",
    "        print(\"Loaded pretrained weights\")\n",
    "        log_dir = \"data-keras_to_snn_params_zanon-1fold-0run-5wc-0.0dp-100bs-200ep-SpikingRectifiedLinearact-5sfr-0.001syn-0.001lr-20230906-210620\"\n",
    "        with nengo_dl.Simulator(converter.net, seed=0) as sim:\n",
    "            sim.load_params(log_dir)   \n",
    "\n",
    "    ## Plot metrics before and after training\n",
    "    print('\\n#-------- Metrics Data Before Training --------#')\n",
    "    print(evaluate_b)\n",
    "\n",
    "\n",
    "    print('\\n#-------- Metrics Data After Training --------#')\n",
    "    print(evaluate_a)\n",
    "\n",
    "\n",
    "    print(\"\\n>>> [DEBUG] History Keys: \")\n",
    "    print(history.history.keys())\n",
    "\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "        \n",
    "    f1_score = history.history['probe_f1_metric']\n",
    "    val_f1_score = history.history['val_probe_f1_metric']\n",
    "    epochs_range = range(len(f1_score))\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, f1_score, label='Training f1_score')\n",
    "    plt.plot(epochs_range, val_f1_score, label='Validation f1_score')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"f1_score\")\n",
    "    plt.title('Training and Validation f1_score')\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # Run predict complete data\n",
    "    \n",
    "    run_predict(converter, log_dir, x_test_, y_test_)\n",
    "    return evaluate_a, elapsed_train_time_m\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(nome_pasta2 + run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    eval_a, train_time = run_train(run_dir, hparams)\n",
    "    print(\"\\n>>> [DEBUG]: \")\n",
    "    print(eval_a)\n",
    "    tf.summary.scalar(METRIC_LOSS, float(eval_a[\"loss\"]), step=1)\n",
    "    tf.summary.scalar(METRIC_F1, float(eval_a[\"probe_f1_metric\"]), step=1)\n",
    "    tf.summary.scalar(TIME, float(train_time), step=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, run (don't train) and load the parameters trained before. Change the parameters to get better results \n",
    "\n",
    "def run_predict(nengo_converter, log_dir, x_test_, y_test_, n_test=365229):\n",
    "\n",
    "    #Adjust data teste\n",
    "    x_dados_test = x_test_\n",
    "    y_dados_test = y_test_\n",
    "    \n",
    "    # get input/output objects\n",
    "    nengo_input = nengo_converter.inputs[nengo_converter.model.layers[0].get_output_at(-1)]\n",
    "    nengo_output = nengo_converter.outputs[nengo_converter.model.output]  \n",
    "    \n",
    "    with nengo_converter.net:\n",
    "        nengo_dl.configure_settings(stateful=False)\n",
    "    \n",
    "    # new simulation to do predict with whatever n_test\n",
    "    with nengo_dl.Simulator(nengo_converter.net, seed=0) as sim:\n",
    "        \n",
    "        sim.load_params(log_dir)\n",
    "        ## Predict: Generate output predictions for the input TEST samples (just with \"n_test\" value)\n",
    "        data = sim.predict(x_dados_test[0:n_test])    \n",
    "    \n",
    "    # The accuracy can be compute like this function below with the specific n_teps, but if you use sim.evalute function\n",
    "    # you compute the accuracy with all TEST data.\n",
    "    # So, compute accuracy on test data, using output of network on last timestep\n",
    "    predictions = np.argmax(data[nengo_output][:, -1], axis=-1)\n",
    "    accuracy = (predictions == y_dados_test[:n_test, 0, 0]).mean()\n",
    "    print(f\"Test accuracy: {100 * accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"\\n>>> [DEBUG]: \")\n",
    "    print(\"Tipo de Dados: \")\n",
    "    dtype_array1 = y_dados_test.dtype\n",
    "    dtype_array2 = predictions.dtype\n",
    "    #print(f\"Tipo de conteúdo do array (y_dados_test, predictions): {dtype_array1, dtype_array2}\")\n",
    "    \n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    \n",
    "    con_matrix = confusion_matrix(y_dados_test[:n_test, 0, 0], predictions)\n",
    "    print(\"Os valores da matriz de confusão são: \\n\", con_matrix)\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    print(\"Dados estatísticos:\")\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    epsilon = 1e-9  # Valor pequeno para suavização/smoothing Laplaciana (garantir não divisão por zero)\n",
    "\n",
    "    print(\"Amostras totais: Normal:\", con_matrix.sum(axis=1)[1], \",\",\n",
    "          \"Pre-Ictal:\", con_matrix.sum(axis=1)[2],\",\",\n",
    "          \"Ictal:\", con_matrix.sum(axis=1)[0])#,\",\",\n",
    "          #\"Post-Ictal\", matriz_confusao_DT.sum(axis=1)[2])\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    FP = (con_matrix.sum(axis=0) - np.diag(con_matrix))\n",
    "\n",
    "    print(\"Falsos-Positivos: Normal:\", FP[1], \",\",\n",
    "          \"Pre-Ictal:\", FP[2],\",\",\n",
    "          \"Ictal:\", FP[0])#,\",\",\n",
    "          #\"Post-Ictal:\", FP[2],'\\n')\n",
    "\n",
    "    FN = con_matrix.sum(axis=1) - np.diag(con_matrix)\n",
    "\n",
    "    print(\"Falsos-Negativos: Normal:\", FN[1], \",\",\n",
    "          \"Pre-Ictal:\", FN[2], \",\",\n",
    "          \"Ictal:\", FN[0])#,\",\",\n",
    "          #\"Post-Ictal:\", FN[2],'\\n')\n",
    "\n",
    "    TP = np.diag(con_matrix)\n",
    "    print(\"Verdadeiros-Positivos: Normal:\", TP[1], \",\",\n",
    "          \"Pre-Ictal:\", TP[2], \",\",\n",
    "          \"Ictal:\", TP[0])#,\",\",\n",
    "          #\"Post-Ictal:\", TP[2],'\\n')\n",
    "\n",
    "    TN = con_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    print(\"Verdadeiros-Negativos: Normal\", TN[1],\n",
    "          \",\",\n",
    "          \"Pre-Ictal:\", TN[2],\n",
    "          \",\",\n",
    "          \"Ictal:\", TN[0])#,\n",
    "          #\",\",\n",
    "         #\"Post-Ictal:\", TN[2])\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN+epsilon) * 100\n",
    "    print(\"Taxa de Verdadeiros Positivos (sensibilidade): Normal: %.2f\" %TPR[1],\"%\", \",\", \"Pre-Ictal: %.2f\" %TPR[2],\"%\",\n",
    "          \",\",\"Ictal: %.2f\" %TPR[0],\"%\")#,\",\",\"Post-Ictal: %.2f\" %TPR[2],\"%\",\"\\n\")\n",
    "\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP+epsilon) * 100\n",
    "    print(\"Taxa de Verdadeiros Negativos (especificidade): Normal: %.2f\" %TNR[1],\"%\", \",\", \"Pre-Ictal: %.2f\" %TNR[2],\"%\",\n",
    "          \",\",\"Ictal: %.2f\" %TNR[0],\"%\")#,\",\",\"Post-Ictal: %.2f\" %TNR[2],\"%\")\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP+epsilon) * 100\n",
    "    print(\"Valor Preditivo Positivo (precisão): Normal: %.2f\" %PPV[1],\"%\", \",\", \"Pre-Ictal: %.2f\" %PPV[2],\"%\", \",\",\n",
    "          \"Ictal: %.2f\" %PPV[0],\"%\")#, \"Post-Ictal: %.2f\" %PPV[2],\"%\",\"\\n\")\n",
    "\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN+epsilon) * 100\n",
    "    print(\"Valor Preditivo Negativo: Normal: %.2f\" %NPV[1],\"%\", \",\", \"Pre-Ictal: %.2f\" %NPV[2],\"%\", \",\",\n",
    "          \"Ictal: %.2f\" %NPV[0],\"%\")#, \"Post-Ictal: %.2f\" %NPV[2],\"%\")\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN+epsilon)\n",
    "    print(\"Taxa de Falsos Positivos: Normal: %.5f\" %FPR[1], \",\", \"Pre-Ictal: %.5f\" %FPR[2], \",\",\n",
    "          \"Ictal: %.5f\" %FPR[0])#, \"Post-Ictal: %.5f\" %FPR[2],\"\\n\")\n",
    "\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN+epsilon)\n",
    "    print(\"Taxa de Falsos Negativos: Normal: %.5f\" %FNR[1], \",\", \"Pre-Ictal: %.5f\" %FNR[2], \",\",\n",
    "          \"Ictal: %.5f\" %FNR[0])#, \"Post-Ictal: %.5f\" %FNR[2],\"\\n\")\n",
    "\n",
    "    # False discovery rate: expressa a proporção esperada de hipóteses nulas rejeitadas erroneamente e possibilita o controle dos falsos positivos\n",
    "    FDR = FP/(TP+FP+epsilon)\n",
    "    print(\"Taxa de descoberta falsa: Normal: %.2f\" %FDR[1],\"%\", \",\", \"Pre-Ictal %.2f\" %FDR[2],\"%\", \",\",\n",
    "          \"Ictal: %.2f\" %FDR[0])#, \"Post-Ictal: %.2f\" %FDR[2],\"%\")\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN+epsilon) * 100\n",
    "    print(\"Taxa de Acertos (acurácia): Normal: %.2f\" %ACC[1],\"%\", \",\", \"Pre-Ictal %.2f\" %ACC[2],\"%\", \",\",\n",
    "          \"Ictal: %.2f\" %ACC[0],\"%\")#, \"Post-Ictal: %.2f\" %ACC[2],\"%\")\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    # Overall F1-Score\n",
    "    F1 = 2*((PPV*TPR)/(PPV+TPR+epsilon))\n",
    "    print(\"Taxa de Acertos (F1-Score): Normal: %.2f\" %F1[1],\"%\", \",\", \"Pre-Ictal %.2f\" %F1[2],\"%\", \",\",\n",
    "          \"Ictal: %.2f\" %F1[0],\"%\")#, \"Post-Ictal: %.2f\" %F1[2],\"%\")\n",
    "    print(\"#-------------------------------------------------------------------------------------------------------#\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir quais são os parâmetros a serem variados (Randomized Search or Grid Search)\n",
    "# Configurar estilo de Search => 0 - Grid Search (Discrete) | 1 - Randomized Search (Interval)\n",
    "\n",
    "# Parâmetros constantes da rede do Sanchotene\n",
    "HP_W_CONSTRAINT = hp.HParam('w_constraint', hp.Discrete([5]))\n",
    "HP_DROPOUT_RATE = hp.HParam('dropout_rate', hp.Discrete([0]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([100])) \n",
    "\n",
    "if type_search_flag == 0:                                                             # Usado no método de grid search\n",
    "    HP_EPOCHS = hp.HParam('epochs', hp.Discrete([200]))                               # 200\n",
    "    HP_ACTIVATION = hp.HParam('activation', hp.Discrete([\"SpikingRectifiedLinear\"])) \n",
    "    HP_SCALE_FIRING_RATES = hp.HParam('firing_rates', hp.Discrete([5]))               # 5\n",
    "    HP_SYNAPSE = hp.HParam('synapse', hp.Discrete([0.001]))                           #0.001\n",
    "    HP_LR = hp.HParam('learning_rate', hp.Discrete([0.001]))                          # 0.001\n",
    "elif type_search_flag == 1:                                                        # Usado no método de randomized search\n",
    "    HP_EPOCHS = hp.HParam('epochs', hp.IntInterval(100, 200))                          # 100, 200\n",
    "    HP_LR = hp.HParam('learning_rate', hp.RealInterval(0.001, 0.01))                  # 0.001, 0.01\n",
    "    HP_ACTIVATION = hp.HParam('activation', hp.Discrete([\"LIF\", \"Izhikevich\", \"SpikingRectifiedLinear\"])) \n",
    "    HP_SCALE_FIRING_RATES = hp.HParam('firing_rates', hp.IntInterval(2, 5))           # 2, 5\n",
    "    HP_SYNAPSE = hp.HParam('synapse', hp.RealInterval(0.001, 0.005))                  #0.001, 0.005\n",
    "\n",
    "\n",
    "## Define as métricas de retorno -> Lembrar que é na predição..\n",
    "METRIC_LOSS = 'loss'\n",
    "METRIC_F1 = 'f1score'\n",
    "TIME = 'time'\n",
    "\n",
    "#[HP_W_CONSTRAINT, HP_DROPOUT_RATE, HP_BATCH_SIZE, HP_BATCH_SIZE, HP_EPOCHS, HP_ACTIVATION, HP_SCALE_FIRING_RATES, HP_SYNAPSE, HP_LR]\n",
    "\n",
    "with tf.summary.create_file_writer(nome_pasta2).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_W_CONSTRAINT, HP_DROPOUT_RATE, HP_BATCH_SIZE, HP_EPOCHS, HP_ACTIVATION, HP_SCALE_FIRING_RATES, HP_SYNAPSE, HP_LR],\n",
    "    metrics=[\n",
    "             hp.Metric(METRIC_LOSS, display_name='Loss'),\n",
    "             hp.Metric(METRIC_F1, display_name='F1-Score'),\n",
    "             hp.Metric(TIME, display_name='Time'),\n",
    "            ])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "aux_acc = np.zeros(0)\n",
    "\n",
    "print(\"\\n#----------------------- CONVERTER SNN TRAINING --------------------------#\")\n",
    "start_train_all = time.time()\n",
    "\n",
    "\n",
    "\n",
    "for train, validation in skfold.split(x_train_val, y_train_val):\n",
    "\n",
    "    print(\"\\n#---------------------------------------------------------#\")\n",
    "    print(\"Fold: \", fold_no)\n",
    "    start_train_fold = time.time()\n",
    "    session_num = 0\n",
    "    \n",
    "    if type_search_flag == 0: # Usa método - Discrete()\n",
    "        for w_constraint in HP_W_CONSTRAINT.domain.values: \n",
    "            for dropout_rate in HP_DROPOUT_RATE.domain.values: \n",
    "                for batch_size in HP_BATCH_SIZE.domain.values: \n",
    "                    for epochs in HP_EPOCHS.domain.values:\n",
    "                        for activation in HP_ACTIVATION.domain.values:\n",
    "                            for scale_firing_rates in HP_SCALE_FIRING_RATES.domain.values: \n",
    "                                for synapse in HP_SYNAPSE.domain.values: \n",
    "                                    for lr in HP_LR.domain.values:\n",
    "                                        hparams = {\n",
    "                                            HP_W_CONSTRAINT: w_constraint, \n",
    "                                            HP_DROPOUT_RATE: dropout_rate, \n",
    "                                            HP_BATCH_SIZE: batch_size,\n",
    "                                            HP_EPOCHS: int(\"%d\"%int(epochs)), # 1 casa\n",
    "                                            HP_ACTIVATION: activation,\n",
    "                                            HP_SCALE_FIRING_RATES: int(\"%d\"%int(scale_firing_rates)), # 1 casa\n",
    "                                            HP_SYNAPSE:float(\"%.3f\"%float(synapse)), # 3 casas\n",
    "                                            HP_LR: float(\"%.3f\"%float(lr)), # 3 casas\n",
    "                                        }\n",
    "    \n",
    "                                        name = \"keras_to_snn_params_zanon-\" + str(fold_no) + \"fold-\" + str(session_num) + \"run-\" + str(hparams[HP_W_CONSTRAINT]) + \"wc-\"+ str(hparams[HP_DROPOUT_RATE]) + \"dp-\" + str(hparams[HP_BATCH_SIZE]) + \"bs-\"+ str(hparams[HP_EPOCHS]) + \"ep-\"+ str(hparams[HP_ACTIVATION]) + \"act-\" + str(hparams[HP_SCALE_FIRING_RATES]) + \"sfr-\" + str(hparams[HP_SYNAPSE]) + \"syn-\" + str(hparams[HP_LR]) + \"lr-\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "                                        run_name = \"run-%d\" % session_num \n",
    "                                        print('Starting trial: %s' % run_name)\n",
    "                                        print({h.name: hparams[h] for h in hparams})\n",
    "                                        run(name, hparams)\n",
    "                                        session_num += 1\n",
    "                                        \n",
    "    elif type_search_flag == 1: # Usa método - Interval()\n",
    "        for w_constraint in HP_W_CONSTRAINT.domain.values: \n",
    "            for dropout_rate in HP_DROPOUT_RATE.domain.values: \n",
    "                for batch_size in HP_BATCH_SIZE.domain.values: \n",
    "                    for epochs in tf.linspace(HP_EPOCHS.domain.min_value, HP_EPOCHS.domain.max_value, 2):\n",
    "                        for activation in HP_ACTIVATION.domain.values:\n",
    "                            for scale_firing_rates in tf.linspace(HP_SCALE_FIRING_RATES.domain.min_value, HP_SCALE_FIRING_RATES.domain.max_value, 2): \n",
    "                                for synapse in tf.linspace(HP_SYNAPSE.domain.min_value, HP_SYNAPSE.domain.max_value, 2): \n",
    "                                    for lr in tf.linspace(HP_LR.domain.min_value, HP_LR.domain.max_value, 2):\n",
    "                                        hparams = {\n",
    "                                            HP_W_CONSTRAINT: w_constraint, \n",
    "                                            HP_DROPOUT_RATE: dropout_rate, \n",
    "                                            HP_BATCH_SIZE: batch_size,\n",
    "                                            HP_EPOCHS: int(\"%d\"%int(epochs)), # 1 casa\n",
    "                                            HP_ACTIVATION: activation,\n",
    "                                            HP_SCALE_FIRING_RATES: int(\"%d\"%int(scale_firing_rates)), # 1 casa\n",
    "                                            HP_SYNAPSE:float(\"%.3f\"%float(synapse)), # 3 casas\n",
    "                                            HP_LR: float(\"%.3f\"%float(lr)), # 3 casas\n",
    "                                        }\n",
    "    \n",
    "                                        name = \"keras_to_snn_params_zanon-\" + str(fold_no) + \"fold-\" + str(session_num) + \"run-\" + str(hparams[HP_W_CONSTRAINT]) + \"wc-\"+ str(hparams[HP_DROPOUT_RATE]) + \"dp-\" + str(hparams[HP_BATCH_SIZE]) + \"bs-\"+ str(hparams[HP_EPOCHS]) + \"ep-\"+ str(hparams[HP_ACTIVATION]) + \"act-\" + str(hparams[HP_SCALE_FIRING_RATES]) + \"sfr-\" + str(hparams[HP_SYNAPSE]) + \"syn-\" + str(hparams[HP_LR]) + \"lr-\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "                                        run_name = \"run-%d\" % session_num \n",
    "                                        print('Starting trial: %s' % run_name)\n",
    "                                        print({h.name: hparams[h] for h in hparams})\n",
    "                                        run(name, hparams)\n",
    "                                        session_num += 1\n",
    "\n",
    "        \n",
    "    elapsed_train_fold = (time.time() - start_train_fold)/60\n",
    "    print(\"\\nTime training fold (min): \", elapsed_train_fold)\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "print(\"\\n#--------------------------------#\")                                \n",
    "elapsed_train_all = (time.time() - start_train_all)/60\n",
    "print(\"\\nTime training all folds (min): \", elapsed_train_all)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs_with_cross_validation_P1/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(model_energy):\n",
    "    print(\"\\n#-------------------------------------------------------------------------------------------------------#\")\n",
    "    print(\"RNA Original Model:\")\n",
    "    model_energy.summary()\n",
    "    energy = keras_spiking.ModelEnergy(model_energy, example_data=np.ones((1500, 3)))\n",
    "    print(\"\\n#-------------------------------------------------------------------------------------------------------#\")\n",
    "    print(\"General\")\n",
    "    energy.summary(print_warnings=False)\n",
    "    \n",
    "    print(\"\\n#-------------------------------------------------------------------------------------------------------#\")\n",
    "    print(\"Estimation energy: CPU X GPU\")\n",
    "    # Detalhado\n",
    "    energy.summary(columns=(\n",
    "        \"name\",\n",
    "        \"rate\", \n",
    "        \"synop_energy cpu\",\n",
    "        \"synop_energy arm\",\n",
    "        \"neuron_energy cpu\",\n",
    "        \"neuron_energy arm\",\n",
    "    ),print_warnings=False, line_length = 100)\n",
    "    #Resumido\n",
    "    print(\"\\n\")\n",
    "    energy.summary(columns=(\"name\", \"energy cpu\", \"energy gpu\"), print_warnings=False)\n",
    "    \n",
    "    print(\"\\n#-------------------------------------------------------------------------------------------------------#\")\n",
    "    print(\"Estimation energy: Loihi X Spinnaker\")\n",
    "    #Detalhado\n",
    "    energy.summary(columns=(\n",
    "        \"name\",\n",
    "        \"rate\",\n",
    "        \"synop_energy loihi\",\n",
    "        \"neuron_energy loihi\",\n",
    "        \"synop_energy spinnaker2\",\n",
    "        \"neuron_energy spinnaker2\",\n",
    "    ),\n",
    "    print_warnings=False, line_length = 120)\n",
    "    #Resumido\n",
    "    print(\"\\n\")\n",
    "    energy.summary(columns=(\"name\", \"energy loihi\", \"energy spinnaker2\"), print_warnings=False)\n",
    "\n",
    "    \n",
    "energy(model)    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "953fa539e6bafcf8e88b6bcfbdaf3858ed475f9073cf5192cef297762647ac84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
